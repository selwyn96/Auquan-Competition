{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet for Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "try:\n",
    "    from urllib2 import urlopen\n",
    "except ImportError:\n",
    "    from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import dcor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data and load into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOAD THE HELPER FUNCTIONS BELOW####\n",
    "#### IMPORTANT: DONOT change these functions or your final submission will not evaluate correctly###\n",
    "\n",
    "## This downloads your datafile, Do not change this function\n",
    "def downloadFile(dataSetId):\n",
    "    fileName = '%s.csv' % (dataSetId)\n",
    "    url = 'https://s3.us-east-2.amazonaws.com/qq10-data/' + fileName\n",
    "    print(url)\n",
    "\n",
    "    response = urlopen(url)\n",
    "    status = response.getcode()\n",
    "    if status == 200:\n",
    "      print('Downloading the dataset %s' % (fileName))\n",
    "      with open(fileName, 'w') as f:\n",
    "          f.write(response.read().decode('utf8'))\n",
    "      return True\n",
    "    else:\n",
    "      logError('File not found. Please ensure you are working with correct data set Id')\n",
    "      return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'feature_data'\n",
    "if not os.path.isfile('%s.csv'%filename):\n",
    "    downloadFile('%s'%filename)\n",
    "df_train = pd.read_csv('%s.csv'%filename)\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'target_variable_data'\n",
    "if not os.path.isfile('%s.csv'%filename):\n",
    "    downloadFile('%s'%filename)\n",
    "y_train = pd.read_csv('%s.csv'%filename)\n",
    "y_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View descriptive statistics of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore a single target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train['A1'].plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_train['A1'], fit = norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train['A1'].skew(), y_train['A1'].kurt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try transformations on the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_a1 = np.log1p(y_train['A1'])\n",
    "sns.distplot(log_a1, fit = norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other descriptive plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.hist(bins=50, figsize=(30,20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore relationships with explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'Alpha_A1_1'\n",
    "data = pd.concat([y_train['A1'], df_train[var]], axis=1)\n",
    "data.plot.scatter(x=var, y='A1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'Beta_A_1'\n",
    "data = pd.concat([y_train['A1'], df_train[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(16, 8))\n",
    "fig = sns.boxplot(x=var, y='A1', data=data)\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "cols = ['Alpha_A1_1', 'Alpha_A1_2', 'Alpha_A1_3', 'Alpha_A1_4', 'Alpha_A1_5', 'Alpha_A1_6', 'Alpha_A1_7', 'Alpha_A1_8', 'Alpha_A1_9', 'Alpha_A1_10']\n",
    "sns.pairplot(df_train.filter(regex='_A1'), size = 2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore intercorrelations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = df_train.filter(regex='_A1').corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'Alpha_A1_1'\n",
    "f, ax = plt.subplots(figsize=(12, 10))\n",
    "k = 25 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, var)[var].index\n",
    "cm = np.corrcoef(df_train[cols].values.T)\n",
    "sns.heatmap(cm, ax=ax, cmap=\"YlGnBu\", linewidths=0.1, yticklabels=cols.values, xticklabels=cols.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = sns.clustermap(cm, cmap=\"YlGnBu\", linewidths=0.1);\n",
    "plt.setp(cg.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
    "cg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore correlations with target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = 'A1'\n",
    "df_tv = df_train.filter(regex='_A1').join(y_train[tv])\n",
    "corrmat = df_tv.corr(method='spearman')\n",
    "f, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "k = 50 #number of variables to explore\n",
    "cols = corrmat.nlargest(k, tv)[tv].index\n",
    "cm = np.corrcoef(df_tv[cols].values.T)\n",
    "sns.heatmap(cm, ax=ax, cmap=\"YlGnBu\", linewidths=0.1, yticklabels=cols.values, xticklabels=cols.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    temp_df = pd.DataFrame(df_tv[cols[i]], index = df_tv.index, columns=[cols[i], tv])\n",
    "    temp_df[tv] = df_tv[tv]\n",
    "    print(temp_df.corr(method='pearson'))\n",
    "    plt.plot(temp_df[cols[i]], temp_df[tv], '.b')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for discarding uncorrelated features from the feature set.( This is a test for feature A1)\n",
    "Var='A1'\n",
    "# for alpha features.\n",
    "features=[]\n",
    "final_features=[]\n",
    "count=0\n",
    "index='Alpha_'+ Var\n",
    "index2='Beta_'+Var[0]\n",
    "index3='Beta_Z_'\n",
    "filter_col = [col for col in df_train if (col.startswith(index) or col.startswith(index2) or col.startswith(index3))]\n",
    "lenght=len(filter_col)\n",
    "for i in range(0,lenght):\n",
    "    combined = pd.concat([df_train[filter_col[i]],y_train[Var]], axis=1)\n",
    "    result=combined.dropna()\n",
    "    correlation=dcor.distance_correlation(result[Var],result[filter_col[i]])\n",
    "    print(correlation,filter_col[i])\n",
    "    if(correlation>0.4):\n",
    "        features.append(filter_col[i])\n",
    "        count=count+1\n",
    "\n",
    "data=pd.DataFrame(df_train, columns = features)\n",
    "print(data)\n",
    "\n",
    "corr = data.corr()\n",
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i,j] >= 0.95 or corr.iloc[i,j] <= -0.95 :\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "selected_features = data.columns[columns]\n",
    "print(selected_features)\n",
    "print(len(selected_features))\n",
    "\n",
    "\n",
    "# Method 2 of discarding correlated features\n",
    "corr = data.corr()\n",
    "m = ~(corr.mask(np.eye(len(corr), dtype=bool)).abs() > 0.90 ).any()\n",
    "print m\n",
    "raw = corr.loc[m, m]\n",
    "print(raw)\n",
    "selected_columns= list(raw)\n",
    "print(selected_columns)\n",
    "print(len(selected_columns))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing the data and performing the feature Engineering \n",
    "import pandas as pd\n",
    "data=pd.read_csv(\"alldata.csv\")\n",
    "col=['A1', 'A2', 'B1', 'B2', 'C1', 'C2', 'D1', 'D2', 'E1', 'a1', 'a2', 'b1', 'b2', 'c1', 'c2', 'd1', 'd2', 'e1']\n",
    "targets= data[col]\n",
    "data.drop(col, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "try:\n",
    "    from urllib2 import urlopen\n",
    "except ImportError:\n",
    "    from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dcor\n",
    "\n",
    "def getCorrelatedFeatures(df_train,target):\n",
    "    #Code for discarding uncorrelated features from the feature set.\n",
    "    temp=list(target)\n",
    "    Var=temp[0]\n",
    "    cap=Var.capitalize()\n",
    "    features=[]\n",
    "    final_features=[]\n",
    "    count=0\n",
    "    index_1='Alpha_'+ cap\n",
    "    index_2='Beta_'+cap[0]\n",
    "    index_3='Beta_Z_'\n",
    "    filter_col = [col for col in df_train if (col.startswith(index_1) or col.startswith(index_2) or col.startswith(index_3))]\n",
    "    lenght=len(filter_col)\n",
    "    # discards features based on Distance Correlation (Features with less than 40% correlation are discarded)\n",
    "    for i in range(0,lenght):\n",
    "        combined = pd.concat([df_train[filter_col[i]],target[Var]], axis=1)\n",
    "        result=combined.dropna()\n",
    "        correlation=dcor.distance_correlation(result[Var],result[filter_col[i]])\n",
    "        if(correlation>0.2):\n",
    "            features.append(filter_col[i])\n",
    "            count=count+1\n",
    "\n",
    "    template=pd.DataFrame(df_train, columns = features)\n",
    "\n",
    "\n",
    "    # The code below discards all Collinear features.(First sumbmission)\n",
    "    corr = template.corr()\n",
    "    columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i+1, corr.shape[0]):\n",
    "            if corr.iloc[i,j] >= 0.95 or corr.iloc[i,j] <= -0.95 :\n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "    selected_features = template.columns[columns]\n",
    "    return(selected_features)\n",
    " \n",
    "''' # Third submission\n",
    "    data=pd.DataFrame(df_train, columns = features)\n",
    "    corr = data.corr()\n",
    "    m = ~(corr.mask(np.eye(len(corr), dtype=bool)).abs() > 0.90 ).any()\n",
    "    raw = corr.loc[m, m]\n",
    "    selected_columns= list(raw)\n",
    "    return(selected_columns)\n",
    "    '''\n",
    " #return(features) Second Submission\n",
    "target=[]\n",
    "target=targets[['A1']]\n",
    "main_features= getCorrelatedFeatures(data,target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import keras\n",
    "import tensorflow\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM, GRU, Conv1D,LocallyConnected1D\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.optimizers import Adam\n",
    "from matplotlib import pyplot as pyplot\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X=pd.DataFrame(data, columns = main_features)\n",
    "Y=target\n",
    "no_features=len(main_features)\n",
    "X_train=X.head(200)\n",
    "Y_train=Y.head(200)\n",
    "X_test=X.tail(40)\n",
    "Y_test=Y.tail(40)\n",
    "\n",
    "X_train=X_train.values\n",
    "X_test=X_test.values\n",
    "Y_train=Y_train.values\n",
    "Y_test=Y_test.values\n",
    "\n",
    "\n",
    "\n",
    "numpy.random.seed(1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(150, bias_initializer='zeros', activation='relu',input_dim=no_features))\n",
    "model.add(Dense(80, bias_initializer='zeros', activation='relu'))\n",
    "model.add(Dense(40, bias_initializer='zeros', activation='relu'))\n",
    "model.add(Dense(15, bias_initializer='zeros', activation='tanh'))\n",
    "model.add(Dense(1, bias_initializer='zeros', activation='linear'))\n",
    "\n",
    "   \n",
    "# compile\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer) #, metrics=['accuracy'])\n",
    "    \n",
    "# fit\n",
    "model.fit(X_train,\n",
    "Y_train, epochs=200, batch_size=50)\n",
    "        \n",
    "   \n",
    "# evaluate\n",
    "score = model.evaluate(X_test, Y_test, batch_size=10)\n",
    "predictions = model.predict(X_test, batch_size=10)\n",
    "print(\"evaluation score:\", score)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "R=sklearn.metrics.r2_score(Y_test, predictions)\n",
    "print(R)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
